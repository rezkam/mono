# SQL Storage Backend

This document describes the SQL storage implementation for the Mono service, supporting both PostgreSQL and SQLite.

## Overview

The SQL storage backend provides a robust, ACID-compliant storage layer for the Mono service. It uses:

- **sqlc** for type-safe SQL code generation
- **goose** for database migrations
- **pgx** driver for PostgreSQL
- **modernc.org/sqlite** pure Go driver for SQLite

## Architecture

The SQL storage implementation follows a layered architecture:

```
┌─────────────────────────────────────┐
│   Application Service Layer         │
│   (internal/service)                 │
└──────────────┬──────────────────────┘
               │ core.Storage interface
┌──────────────┴──────────────────────┐
│   Repository Layer                   │
│   (internal/storage/sql/repository)  │
│   - Implements core.Storage          │
│   - Domain model conversion          │
│   - Transaction management           │
└──────────────┬──────────────────────┘
               │
┌──────────────┴──────────────────────┐
│   sqlc Generated Layer               │
│   (internal/storage/sql/sqlcgen)     │
│   - Type-safe query methods          │
│   - Generated from SQL queries       │
└──────────────┬──────────────────────┘
               │
┌──────────────┴──────────────────────┐
│   Database Layer                     │
│   PostgreSQL or SQLite               │
└─────────────────────────────────────┘
```

### Key Components

1. **Connection Layer** (`internal/storage/sql/connection.go`)
   - Database connection setup
   - Connection pooling configuration
   - Automatic migration runner

2. **Repository Layer** (`internal/storage/sql/repository/store.go`)
   - Implements `core.Storage` interface
   - Wraps sqlc-generated code
   - Handles domain model conversion
   - Manages transactions

3. **Generated Layer** (`internal/storage/sql/sqlcgen/`)
   - Auto-generated by sqlc
   - Type-safe query methods
   - Context-aware operations

4. **Migrations** (`internal/storage/sql/migrations/`)
   - SQL migration files
   - Embedded in binary using go:embed
   - Applied automatically at startup

## Configuration

### PostgreSQL Configuration

Set these environment variables to use PostgreSQL:

```bash
export MONO_STORAGE_TYPE=postgres
export MONO_POSTGRES_URL="postgres://user:password@localhost:5432/mono_db?sslmode=disable"
```

**Connection String Format:**
```
postgres://[user]:[password]@[host]:[port]/[database]?[parameters]
```

**Common Parameters:**
- `sslmode=disable` - Disable SSL (development only)
- `sslmode=require` - Require SSL (production)
- `pool_max_conns=25` - Maximum connections

**Example Production URL:**
```
postgres://mono:securepass@db.example.com:5432/mono_db?sslmode=require&pool_max_conns=20
```

### SQLite Configuration

Set these environment variables to use SQLite:

```bash
export MONO_STORAGE_TYPE=sqlite
export MONO_SQLITE_PATH=./mono-data/mono.db
```

**Default Path:** `./mono-data/mono.db`

**SQLite Pragmas:**
The implementation automatically sets these pragmas for optimal performance:
- `journal_mode=WAL` - Write-Ahead Logging for better concurrency
- `busy_timeout=5000` - 5 second timeout for locked database
- `foreign_keys=on` - Enable foreign key constraints

## Running with SQL Storage

### Quick Start with SQLite

```bash
# Build the server
make build

# Run with SQLite (default path)
MONO_STORAGE_TYPE=sqlite ./mono-server

# Run with custom SQLite path
MONO_STORAGE_TYPE=sqlite MONO_SQLITE_PATH=/data/mono.db ./mono-server
```

### Quick Start with PostgreSQL

```bash
# Start PostgreSQL using Docker
make db-up

# Run the server
MONO_STORAGE_TYPE=postgres \
MONO_POSTGRES_URL="postgres://mono:mono_password@localhost:5432/mono_db" \
./mono-server
```

## Database Connection Pooling

The implementation uses connection pooling with these defaults:

```go
MaxOpenConns:     25  // Maximum open connections
MaxIdleConns:     5   // Maximum idle connections
ConnMaxLifetime:  5m  // Maximum connection lifetime
ConnMaxIdleTime:  1m  // Maximum idle time for connections
```

These settings are optimized for typical web application workloads and can handle:
- ~25 concurrent requests
- Efficient connection reuse
- Automatic cleanup of stale connections

## Context Support

All database operations support `context.Context` for:
- **Cancellation**: Cancel long-running queries
- **Timeouts**: Set query timeouts
- **Tracing**: Propagate trace context (with OpenTelemetry)

Example with timeout:

```go
ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
defer cancel()

list, err := store.GetList(ctx, "list-id")
```

## Data Model Mapping

### TodoList Storage

```go
// Core Domain Model
type TodoList struct {
    ID         string
    Title      string
    Items      []TodoItem
    CreateTime time.Time
}

// Database Tables
// - todo_lists: Main list data
// - todo_items: Associated items (foreign key)
```

### Field Mappings

| Domain Field | Database Field | Type | Notes |
|-------------|----------------|------|-------|
| ID | id | TEXT | UUID string |
| Title | title | TEXT | List name |
| CreateTime | create_time | TIMESTAMP | Auto-set |
| Items[].ID | todo_items.id | TEXT | UUID string |
| Items[].Completed | todo_items.completed | INTEGER | 0=false, 1=true |
| Items[].Tags | todo_items.tags | TEXT | JSON array as string |
| Items[].DueTime | todo_items.due_time | TIMESTAMP | Nullable |

## Development

### Regenerating sqlc Code

After modifying SQL queries in `internal/storage/sql/queries/`:

```bash
make gen-sqlc
```

This regenerates Go code in `internal/storage/sql/sqlcgen/`.

### Creating New Migrations

```bash
NAME=add_priority_column make db-migrate-create
```

Edit the generated migration file in `internal/storage/sql/migrations/`.

### Testing

Run SQL storage tests:

```bash
# SQLite tests (always run)
make test-sql

# PostgreSQL tests (requires TEST_POSTGRES_URL)
make db-up
TEST_POSTGRES_URL="postgres://mono:mono_password@localhost:5432/mono_db" go test -v ./internal/storage/sql/...
```

## Production Considerations

### PostgreSQL Production Setup

1. **Use Connection Pooling**: Already configured in the implementation
2. **Enable SSL**: Use `sslmode=require` in production
3. **Set Proper Permissions**: Create dedicated database user
4. **Regular Backups**: Use pg_dump or continuous archiving
5. **Monitor Connections**: Watch for connection leaks
6. **Tune PostgreSQL**: Adjust shared_buffers, work_mem, etc.

### SQLite Production Setup

1. **Use WAL Mode**: Automatically enabled
2. **Regular Backups**: Use SQLite backup API or file copy (while stopped)
3. **File Permissions**: Ensure database file is properly protected
4. **Disk Space**: Monitor disk usage
5. **Consider Limitations**: 
   - Not suitable for high-concurrency writes
   - Great for read-heavy workloads
   - Perfect for edge deployments

### When to Use Each Database

**Use PostgreSQL when:**
- High concurrency (many simultaneous writes)
- Large datasets (>100GB)
- Multiple application instances
- Need advanced features (JSONB, full-text search)
- Cloud/distributed deployment

**Use SQLite when:**
- Single instance deployment
- Edge computing / IoT
- Desktop applications
- Development/testing
- Read-heavy workloads
- Simple deployment (single file)

## Trade-offs and Limitations

### Cross-Database Compatibility

To support both PostgreSQL and SQLite, we make these trade-offs:

1. **No JSONB**: Use TEXT for JSON storage
   - Pro: Works on both databases
   - Con: No native JSON indexing in PostgreSQL
   - Impact: Minimal for small tag arrays

2. **INTEGER for Booleans**: Use 0/1 instead of BOOLEAN
   - Pro: Explicit and compatible
   - Con: Slightly verbose
   - Impact: None in practice

3. **Simple Indexes**: B-tree indexes only
   - Pro: Supported everywhere
   - Con: Can't use PostgreSQL-specific indexes (GIN, GiST)
   - Impact: Fine for our schema

4. **TEXT Primary Keys**: Use TEXT for UUIDs instead of UUID type
   - Pro: Universal compatibility
   - Con: Slightly more storage in PostgreSQL
   - Impact: Negligible

### Performance Characteristics

**SQLite:**
- Fast reads (~1000 ops/sec for simple queries)
- Good writes (~500 ops/sec)
- Single writer at a time (WAL mode helps)
- Entire database in memory option available

**PostgreSQL:**
- Very fast reads (10,000+ ops/sec)
- Fast writes (5,000+ ops/sec)
- Multiple concurrent writers
- Scales horizontally with replication

## Troubleshooting

### Common Issues

**Issue: "Failed to connect to database"**
- Check connection string format
- Verify database is running
- Check network/firewall settings
- Verify credentials

**Issue: "Migration failed"**
- Check database permissions
- Review migration SQL syntax
- Ensure no manual schema changes
- Check goose_db_version table

**Issue: "Too many open connections"**
- Increase MaxOpenConns if needed
- Check for connection leaks
- Monitor connection usage
- Verify connections are closed

**Issue: "Database is locked" (SQLite)**
- Another process has the database open
- Long-running transaction
- Increase busy_timeout
- Consider PostgreSQL for high concurrency

### Debug Mode

Enable SQL query logging:

```bash
# PostgreSQL - use log_statement in postgresql.conf
# or set in connection string:
postgres://user:pass@host/db?log_statement=all

# SQLite - limited built-in logging
# Use PRAGMA commands or external tools
```

## Migration from Other Storage Backends

### From Filesystem Storage

To migrate from FS to SQL storage:

1. Export data while FS storage is active
2. Switch to SQL storage
3. Import data using API

(Automated migration tool can be developed if needed)

### From GCS Storage

Same process as filesystem migration.

## References

- [sqlc Documentation](https://docs.sqlc.dev/)
- [goose Documentation](https://github.com/pressly/goose)
- [pgx PostgreSQL Driver](https://github.com/jackc/pgx)
- [modernc.org/sqlite](https://gitlab.com/cznic/sqlite)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [SQLite Documentation](https://www.sqlite.org/docs.html)
